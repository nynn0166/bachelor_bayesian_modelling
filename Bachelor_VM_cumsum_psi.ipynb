{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413092d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import math\n",
    "from math import pi\n",
    "import time\n",
    "\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from jax import numpy as jnp, random\n",
    "\n",
    "import numpyro\n",
    "from numpyro.distributions import (\n",
    "    Beta,\n",
    "    Categorical,\n",
    "    Dirichlet,\n",
    "    Gamma,\n",
    "    Normal,\n",
    "    SineSkewed,\n",
    "    Uniform,\n",
    "    VonMises,\n",
    "    HalfNormal\n",
    ")\n",
    "from numpyro.infer import MCMC, NUTS, Predictive, init_to_value\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numpyro.infer.util import log_likelihood\n",
    "from numpyro.handlers import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2f1215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(num_mix_comp, data = None, n = None, identifiable=False, kappa_max=1000):\n",
    "    if data is None:\n",
    "        # We are predicting - no data\n",
    "        assert(not n is None)\n",
    "    else:\n",
    "        # We are training - we have data\n",
    "        assert(n is None)\n",
    "        n = data.shape[0]\n",
    "        assert(data.shape == (n,2))\n",
    "\n",
    "    # Sampling mixture weights from a Dirichlet distribution (latent variable)\n",
    "    mix_weights = numpyro.sample(\"mix_weights\", Dirichlet(jnp.ones((num_mix_comp,))))\n",
    "\n",
    "    if identifiable:\n",
    "    # We constrain psi_loc so that it increases with mixture component m\n",
    "        # Note the +1, this is because we will use cumsum\n",
    "        edges = numpyro.sample(\"edges\", Dirichlet(jnp.ones((num_mix_comp+1,))))\n",
    "        cumsum = jnp.cumsum(edges)\n",
    "        # Remove last dim in the cumsum because it is always 1\n",
    "        cumsum = cumsum[:-1]\n",
    "        psi_loc = 2*jnp.pi*cumsum - jnp.pi\n",
    "\n",
    "    # Sample mixture rvs\n",
    "    # Using plate to indicate that the following variables are conditionally independent\n",
    "    with numpyro.plate(\"mixture\", num_mix_comp):\n",
    "        # Sampling mixture component parameters for von Mises distributions\n",
    "        # Locations\n",
    "        phi_loc = numpyro.sample(\"phi_loc\", Uniform(-jnp.pi, jnp.pi)) # Mean direction for phi\n",
    "\n",
    "        if not identifiable:\n",
    "            # No constraints\n",
    "            psi_loc = numpyro.sample(\"psi_loc\", Uniform(-jnp.pi, jnp.pi)) # Mean direction for psi\n",
    "\n",
    "        psi_conc = numpyro.sample(\"psi_conc\", Uniform(0.1, kappa_max))\n",
    "        phi_conc = numpyro.sample(\"phi_conc\", Uniform(0.1, kappa_max))\n",
    "\n",
    "    # Combine the locs and kappas for a single VonMises likelihood\n",
    "    locs = jnp.stack((phi_loc, psi_loc), -1)\n",
    "    assert(locs.shape==(num_mix_comp,2))\n",
    "    kappas = jnp.stack((phi_conc, psi_loc), -1)\n",
    "    assert(kappas.shape==(num_mix_comp,2))\n",
    "\n",
    "    # Using plate for the observed data\n",
    "    with numpyro.plate(\"data_plate\", n):\n",
    "        # Sampling the mixture component assignment (latent variable)\n",
    "        assign = numpyro.sample(\"mix_comp\", Categorical(mix_weights), infer={\"enumerate\": \"parallel\"})\n",
    "\n",
    "        locs_a = locs[assign]\n",
    "        kappas_a = kappas[assign]\n",
    "\n",
    "        # to_event is used because the VM is univariate, but we have 2D data\n",
    "        von = VonMises(locs_a, kappas_a).to_event(1)\n",
    "\n",
    "        # Define the likelihood \n",
    "        phi_psi = numpyro.sample(\"phi_psi\", von, obs = data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f27c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Hamiltonian Monte Carlo\n",
    "def run_hmc(rng_key, model, data, num_mix_comp, args):\n",
    "    from numpyro.infer import init_to_mean\n",
    "    kernel = NUTS(model, init_strategy=init_to_mean())\n",
    "    # TH: two chains \n",
    "    mcmc = MCMC(kernel, num_chains=2, num_samples=args.num_samples, num_warmup=args.num_warmup)\n",
    "    # TH: fixed this\n",
    "    mcmc.run(rng_key, num_mix_comp=num_mix_comp, n=None, data=data, identifiable=args.identifiable)\n",
    "    mcmc.print_summary()\n",
    "    post_samples = mcmc.get_samples()\n",
    "\n",
    "    def waic(model, posterior, model_args, model_kwargs):\n",
    "        waic_result = az.waic(get_idata(model, posterior, model_args, model_kwargs))\n",
    "        elpd_waic = waic_result.elpd_waic\n",
    "        p_waic = waic_result.p_waic\n",
    "        return elpd_waic\n",
    "\n",
    "    def loo(model, posterior, model_args, model_kwargs):\n",
    "        loo_result = az.loo(get_idata(model, posterior, model_args, model_kwargs))\n",
    "        elpd_loo = loo_result.elpd_loo\n",
    "        p_loo = loo_result.p_loo\n",
    "        return elpd_loo\n",
    "\n",
    "    def ess(model, posterior, model_args, model_kwargs):\n",
    "        return az.ess(get_idata(model, posterior, model_args, model_kwargs))\n",
    "\n",
    "    def rhat(model, posterior, model_args, model_kwargs):\n",
    "        return az.rhat(get_idata(model, posterior, model_args, model_kwargs))\n",
    "\n",
    "\n",
    "     # We need to sample the assignments, looks az.from_numpyro doesn't handle discrete sites correctly.\n",
    "     # **NOTE**: I have not checked whether we should enumerate mix_comp when computing WAIC.\n",
    "    def get_idata(model, posterior, model_args, model_kwargs):\n",
    "        ll = log_likelihood(seed(model, rng_seed=0), posterior, *model_args, **model_kwargs)\n",
    "        ll = {k: v[None] for k, v in ll.items()}\n",
    "        idata = az.convert_to_inference_data(\n",
    "            {k: v[None] for k, v in posterior.items() if k not in ll}\n",
    "        )\n",
    "        idata.add_groups(log_likelihood=ll)\n",
    "        return idata\n",
    "\n",
    "    expected_waic = waic(model, post_samples, (num_mix_comp, data), {})\n",
    "    expected_loo = loo(model, post_samples, (num_mix_comp, data), {})\n",
    "    expected_ess = ess(model, post_samples, (num_mix_comp, data), {})\n",
    "    expected_rhat = rhat(model, post_samples, (num_mix_comp, data), {})\n",
    "\n",
    "    return post_samples, expected_waic, expected_loo, expected_ess, expected_rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "956ae18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_vs_components(metric_values, num_components, metric_name, iters):\n",
    "    metric_values = metric_values\n",
    "    print(f\"{metric_name} metric_vals : \", metric_values)\n",
    "    print(f\"{metric_name} num_comp : \", num_components)\n",
    "    plt.figure()\n",
    "    plt.plot(num_components, metric_values, marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f'{metric_name} vs Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    save_path = f'model1_{metric_name}_vs_components_{iters}.png'\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Calculate and print the optimal component\n",
    "    if len(num_components) > 1:\n",
    "        optimal_component = num_components[np.argmax(metric_values)]\n",
    "        print(f'Optimal {metric_name} Component: {optimal_component}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e19f302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rama(phi_values, psi_values):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    ax.set_title(\"Ramachandran Plot\")\n",
    "\n",
    "    mask = ~np.isnan(phi_values) & ~np.isnan(psi_values)\n",
    "    phi_values = phi_values[mask]\n",
    "    psi_values = psi_values[mask]\n",
    "\n",
    "    # Create hexbin plot \n",
    "    hb = ax.hexbin(phi_values, psi_values, bins='log', gridsize=50, cmap='inferno')\n",
    "   \n",
    "    # Set labels and colorbar\n",
    "    ax.set_xlabel('Phi Angles')\n",
    "    ax.set_ylabel('Psi Angles')\n",
    "    cb = fig.colorbar(hb, ax=ax, label='Counts')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efefaec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(phi_values, psi_values, iters):\n",
    "\n",
    "    # Check if the lengths of phi_values and psi_values match\n",
    "    if len(phi_values) != len(psi_values):\n",
    "        raise ValueError(\"The lengths of phi_values and psi_values must be the same.\")\n",
    "\n",
    "    # Create a DataFrame from the phi_values and psi_values\n",
    "    data = pd.DataFrame({'Phi': phi_values, 'Psi': psi_values})\n",
    "\n",
    "    # Create a density plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.kdeplot(data=data, x='Phi', y='Psi', fill=True, cmap='viridis', cbar=True)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Phi\")\n",
    "    plt.ylabel(\"Psi\")\n",
    "    plt.title(\"Phi vs. Psi Angles Density\")\n",
    "    save_path = f'model1_density_{iters}.png'\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56d05e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_concentrations(post_samples):\n",
    "    phi_concentration = post_samples['phi_conc']\n",
    "    psi_concentration = post_samples['psi_conc']\n",
    "\n",
    "    # Fixing random state for reproducibility\n",
    "    np.random.seed(19680801)\n",
    "\n",
    "    x = np.array(phi_concentration)\n",
    "    y = np.array(psi_concentration)\n",
    "    xlim = (x.min(), x.max())\n",
    "    ylim = (y.min(), y.max())\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    hb = plt.hexbin(x, y, gridsize=50, bins='log', cmap='inferno')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.title(\"Phi vs Psi Concentrations\")\n",
    "    cb = plt.colorbar(hb, label='counts')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db0c8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_values(result):\n",
    "    values = []\n",
    "    for var in result.data_vars:\n",
    "        values.extend(result[var].values.flatten())\n",
    "    return np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e40c1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load data from the file\n",
    "    N = 1000  # Limit the data set size for testing\n",
    "    data = jnp.array(np.load(\"top500_inliers.npy\")[0:N])\n",
    "    assert data.shape == (N, 2)\n",
    "\n",
    "    phi = data[:, 0]\n",
    "    psi = data[:, 1]\n",
    "    \n",
    "    # Initialize lists to accumulate the ESS and R-hat values\n",
    "    accumulated_ess_values = []\n",
    "    accumulated_rhat_values = []\n",
    "\n",
    "    # Initialize lists to store the results for other metrics\n",
    "    all_num_mix_comp = []\n",
    "    all_expected_waic = []\n",
    "    all_expected_loo = []\n",
    "    pred_datas = {}\n",
    "\n",
    "    # Define the loop parameters\n",
    "    start = 5\n",
    "    iters = 30\n",
    "    step = 5\n",
    "    rng_key = random.PRNGKey(0)  # Initialize your random key here\n",
    "\n",
    "    # Loop over the specified range\n",
    "    for m in range(start, iters, step):\n",
    "        # Split the random key into multiple keys for different purposes\n",
    "        rng_key, inf_key, pred_key = random.split(rng_key, 3)\n",
    "\n",
    "        # Determine the number of mixture components for this iteration\n",
    "        num_mix_comp = m\n",
    "\n",
    "        # Time the HMC sampling\n",
    "        hmc_start = time.time()\n",
    "        posterior_samples, expected_waic, expected_loo, expected_ess, expected_rhat = run_hmc(inf_key, model, data, num_mix_comp, args)\n",
    "        hmc_end = time.time()\n",
    "        print(f\"HMC sampling for {num_mix_comp} components took {hmc_end - hmc_start:.2f} seconds\")\n",
    "\n",
    "        # Time the predictive sampling\n",
    "        predictive_start = time.time()\n",
    "        predictive = Predictive(model, posterior_samples, parallel=True)\n",
    "        pred_samples = []\n",
    "        num_predictions = 50  # Number of different prediction sets to generate\n",
    "        for _ in range(num_predictions):\n",
    "            pred_key, new_pred_key = random.split(pred_key)\n",
    "            pred_samples.append(predictive(new_pred_key, num_mix_comp=num_mix_comp, data=None, n=N, identifiable=args.identifiable)[\"phi_psi\"])\n",
    "        pred_datas[m] = jnp.concatenate(pred_samples, axis=0)\n",
    "        predictive_end = time.time()\n",
    "        print(f\"Predictive sampling for {num_mix_comp} components took {predictive_end - predictive_start:.2f} seconds\")\n",
    "\n",
    "        # Extract and accumulate ESS and R-hat values\n",
    "        accumulated_ess_values.extend(extract_values(expected_ess))\n",
    "        accumulated_rhat_values.extend(extract_values(expected_rhat))\n",
    "\n",
    "        # Store the values for plotting or further analysis\n",
    "        all_num_mix_comp.append(num_mix_comp)\n",
    "        all_expected_waic.append(expected_waic)\n",
    "        all_expected_loo.append(expected_loo)\n",
    "\n",
    "\n",
    "    # Time the plotting\n",
    "    plotting_start = time.time()\n",
    "    plot_metric_vs_components(all_expected_waic, all_num_mix_comp, \"WAIC\", iters)\n",
    "    plot_metric_vs_components(all_expected_loo, all_num_mix_comp, \"LOO\", iters)\n",
    "\n",
    "    for m in range(start, iters, step):\n",
    "        phi_values = pred_datas[m][:, 0]\n",
    "        psi_values = pred_datas[m][:, 1]\n",
    "        plot_rama(phi_values, psi_values)\n",
    "    \n",
    "    plot_rama(phi, psi)\n",
    "    plot_concentrations(posterior_samples)\n",
    "    plotting_end = time.time()\n",
    "    print(f\"Plotting took {plotting_end - plotting_start:.2f} seconds\")\n",
    "    \n",
    "    # After the loop, compute the std of ESS and mean of R-hat\n",
    "    accumulated_ess_values = np.array(accumulated_ess_values)\n",
    "    accumulated_rhat_values = np.array(accumulated_rhat_values)\n",
    "\n",
    "    std_ess = np.std(accumulated_ess_values)\n",
    "    mean_rhat = np.mean(accumulated_rhat_values[~np.isnan(accumulated_rhat_values)])  # Filter out NaN values\n",
    "\n",
    "    print(f\"Overall Standard Deviation of ESS: {std_ess}\")\n",
    "    print(f\"Overall Mean of R-hat: {mean_rhat}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef566481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nynne\\AppData\\Local\\Temp\\ipykernel_54876\\2823958251.py:6: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  mcmc = MCMC(kernel, num_chains=2, num_samples=args.num_samples, num_warmup=args.num_warmup)\n",
      "warmup:   5%|█▎                         | 71/1500 [00:18<12:22,  1.93it/s, 1023 steps of size 1.91e-03. acc. prob=0.74]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Von mises mixture model\"\n",
    "    )\n",
    "    parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n",
    "    parser.add_argument(\"--num-warmup\", nargs=\"?\", default=500, type=int)\n",
    "    parser.add_argument(\"--rng_seed\", type=int, default=123)\n",
    "    parser.add_argument(\"--device\", default=\"gpu\", type=str, help='use \"cpu\" or \"gpu\".')\n",
    "    parser.add_argument(\"--file-path\", type=str, default=\"top500.txt\", help=\"Path to the data file.\")\n",
    "    parser.add_argument(\"--identifiable\", dest=\"identifiable\", default=False, action=\"store_true\", help=\"Identifiable or not.\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
