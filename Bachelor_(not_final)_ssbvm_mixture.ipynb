{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "import matplotlib.colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from jax import numpy as jnp, random\n",
    "\n",
    "import numpyro\n",
    "from numpyro.distributions import (\n",
    "    Beta,\n",
    "    Categorical,\n",
    "    Dirichlet,\n",
    "    Gamma,\n",
    "    Normal,\n",
    "    SineSkewed,\n",
    "    Uniform,\n",
    "    VonMises,\n",
    "    HalfNormal\n",
    ")\n",
    "from numpyro.infer import MCMC, NUTS, Predictive, init_to_value\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from numpyro.infer.util import log_likelihood\n",
    "from numpyro.handlers import seed\n",
    "\n",
    "from directional import SineBivariateVonMises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, num_data, num_mix_comp=2):\n",
    "    # Sampling mixture weights from a Dirichlet distribution (latent variable)\n",
    "    mix_weights = numpyro.sample(\"mix_weights\", Dirichlet(jnp.ones((num_mix_comp,))))\n",
    "    \n",
    "    # Using plate to indicate that the following variables are conditionally independent\n",
    "    with numpyro.plate(\"mixture\", 2):\n",
    "        # Sampling mixture component locations and concentrations (latent variables)\n",
    "        phi_loc = numpyro.sample(\"phi_loc\", Uniform(-1.0*jnp.pi, 1.0*jnp.pi)) #Uniform 0 2pi\n",
    "        psi_loc = numpyro.sample(\"psi_loc\", Uniform(-1.0*jnp.pi, 1.0*jnp.pi))\n",
    "        phi_conc = numpyro.sample(\"phi_conc\", Uniform(1, 1000))\n",
    "        psi_conc = numpyro.sample(\"psi_conc\", Uniform(1, 1000))\n",
    "        corr_scale = numpyro.sample(\"corr_scale\", Beta(2.0, 10.0))\n",
    "    \n",
    "    # Using plate for the observed data\n",
    "    with numpyro.plate(\"obs_plate\", 3000, dim=-1):\n",
    "        # Sampling the mixture component assignment (latent variable)\n",
    "        assign = numpyro.sample(\"mix_comp\", Categorical(mix_weights), infer={\"enumerate\": \"parallel\"})\n",
    "        \n",
    "        # Define the likelihood using the SineBivariateVonMises distribution\n",
    "        sine = SineBivariateVonMises( #NEW CODE\n",
    "            phi_loc=phi_loc[assign],\n",
    "            psi_loc=psi_loc[assign],\n",
    "            phi_concentration=phi_conc[assign],# rama # c1 against c2 ## density correlation, density plot.\n",
    "            psi_concentration=psi_conc[assign],\n",
    "            weighted_correlation=corr_scale[assign]  \n",
    "        )\n",
    "        \n",
    "        # Sampling the observed data (not latent because obs is set)\n",
    "        numpyro.sample(\"phi_psi\", sine, obs=data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2(data, num_data, num_mix_comp=2):\n",
    "    # Sampling mixture weights from a Dirichlet distribution (latent variable)\n",
    "    mix_weights = numpyro.sample(\"mix_weights\", Dirichlet(jnp.ones((num_mix_comp,))))\n",
    "    \n",
    "    \n",
    "    # Using plate to indicate that the following variables are conditionally independent\n",
    "    with numpyro.plate(\"mixture\", num_mix_comp):\n",
    "        # Sampling mixture component locations and concentrations (latent variables)\n",
    "        phi_loc = numpyro.sample(\"phi_loc\", Uniform(-1.0*jnp.pi, 1.0*jnp.pi))\n",
    "        psi_loc = numpyro.sample(\"psi_loc\", Uniform(-1.0*jnp.pi, 1.0*jnp.pi))\n",
    "        # Sampling s_phi and transforming to rho_phi (latent variable)\n",
    "        s_phi = numpyro.sample(\"s_phi\", HalfNormal(1.0))\n",
    "        phi_conc = 1.0 / (s_phi + 0.001)  # Derived, not directly sampled\n",
    "        # Sampling s_psi and transforming to rho_psi (latent variable)\n",
    "        s_psi = numpyro.sample(\"s_psi\", HalfNormal(1.0))\n",
    "        psi_conc = 1.0 / (s_psi + 0.001)  # Derived, not directly sampled\n",
    "        corr_scale = numpyro.sample(\"corr_scale\", Beta(2.0, 10.0))\n",
    "    \n",
    "    # Using plate for the observed data\n",
    "    with numpyro.plate(\"obs_plate\", 100, dim=-1):\n",
    "        # Sampling the mixture component assignment (latent variable)\n",
    "        assign = numpyro.sample(\"mix_comp\", Categorical(mix_weights), infer={\"enumerate\": \"parallel\"})\n",
    "        \n",
    "        # Define the likelihood using the SineBivariateVonMises distribution\n",
    "        sine = SineBivariateVonMises(\n",
    "            phi_loc=phi_loc[assign],\n",
    "            psi_loc=psi_loc[assign],\n",
    "            phi_concentration=phi_conc[assign],\n",
    "            psi_concentration=psi_conc[assign],\n",
    "            weighted_correlation=corr_scale[assign]\n",
    "        )\n",
    "        \n",
    "        # Sampling the observed data (not latent because obs is set)\n",
    "        numpyro.sample(\"phi_psi\", sine, obs=data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Hamiltonian Monte Carlo\n",
    "def run_hmc(rng_key, model, data, num_mix_comp, args):\n",
    "    kernel = NUTS(model)\n",
    "    mcmc = MCMC(kernel, num_samples=args.num_samples, num_warmup=args.num_warmup)\n",
    "    mcmc.run(rng_key, data, len(data), num_mix_comp)\n",
    "    mcmc.print_summary()\n",
    "    post_samples = mcmc.get_samples()\n",
    "    \n",
    "    def waic(model, posterior, model_args, model_kwargs):\n",
    "        waic_result = az.waic(get_idata(model, posterior, model_args, model_kwargs))\n",
    "        elpd_waic = waic_result.elpd_waic\n",
    "        p_waic = waic_result.p_waic\n",
    "        return elpd_waic\n",
    "    \n",
    "    def loo(model, posterior, model_args, model_kwargs):\n",
    "        loo_result = az.loo(get_idata(model, posterior, model_args, model_kwargs))\n",
    "        elpd_loo = loo_result.elpd_loo\n",
    "        p_loo = loo_result.p_loo\n",
    "        return elpd_loo\n",
    "    \n",
    "    def ess(model, posterior, model_args, model_kwargs):\n",
    "        return az.ess(get_idata(model, posterior, model_args, model_kwargs))\n",
    "    \n",
    "    def rhat(model, posterior, model_args, model_kwargs):\n",
    "        return az.rhat(get_idata(model, posterior, model_args, model_kwargs))\n",
    "\n",
    "\n",
    "     # We need to sample the assignments, looks az.from_numpyro doesn't handle discrete sites correctly.\n",
    "     # **NOTE**: I have not checked whether we should enumerate mix_comp when computing WAIC.\n",
    "    def get_idata(model, posterior, model_args, model_kwargs):\n",
    "        ll = log_likelihood(seed(model, rng_seed=0), posterior, *model_args, **model_kwargs)\n",
    "        ll = {k: v[None] for k, v in ll.items()}\n",
    "        idata = az.convert_to_inference_data(\n",
    "            {k: v[None] for k, v in posterior.items() if k not in ll}\n",
    "        )\n",
    "        idata.add_groups(log_likelihood=ll)\n",
    "        return idata\n",
    "   \n",
    "    expected_waic = waic(model, post_samples, (data, len(data)), {})\n",
    "    expected_loo = loo(model, post_samples, (data, len(data)), {})\n",
    "    expected_ess = ess(model, post_samples, (data, len(data)), {})\n",
    "    expected_rhat = rhat(model, post_samples, (data, len(data)), {})\n",
    "        \n",
    "    return post_samples, expected_waic, expected_loo, expected_ess, expected_rhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    phi = []\n",
    "    psi = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue  # Skip comments\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4:\n",
    "                if 'NT' in parts[-2:] or 'CT' in parts[-2:]:\n",
    "                    continue  # Skip lines with 'NT' or 'CT'\n",
    "                phi_values = float(parts[-2])\n",
    "                psi_values = float(parts[-1])\n",
    "                phi.append(phi_values)\n",
    "                psi.append(psi_values)\n",
    "    phi_array = jnp.array(phi)\n",
    "    psi_array = jnp.array(psi)\n",
    "    return [phi_array, psi_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_vs_components(metric_values, num_components, metric_name, iters):\n",
    "    metric_values = metric_values\n",
    "    print(f\"{metric_name} metric_vals : \", metric_values)\n",
    "    print(f\"{metric_name} num_comp : \", num_components)\n",
    "    plt.figure()\n",
    "    plt.plot(num_components, metric_values, marker='o')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f'{metric_name} vs Number of Components')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    save_path = f'model1_{metric_name}_vs_components_{iters}.png'\n",
    "    plt.savefig(save_path)\n",
    "    \n",
    "    # Calculate and print the optimal component\n",
    "    if len(num_components) > 1:\n",
    "        optimal_component = num_components[np.argmax(metric_values)]\n",
    "        print(f'Optimal {metric_name} Component: {optimal_component}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rama(phi_values, psi_values):\n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    ax.set_title(\"Ramachandran Plot\")\n",
    "    \n",
    "    # Create hexbin plot with specified norm and bins\n",
    "    hb = ax.hexbin(phi_values, psi_values, gridsize=50, bins=\"log\", cmap='inferno')\n",
    "    \n",
    "    # Set labels and colorbar\n",
    "    ax.set_xlabel('Phi Angles')\n",
    "    ax.set_ylabel('Psi Angles')\n",
    "    cb = fig.colorbar(hb, ax=ax, label='Counts')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(phi_values, psi_values, iters):\n",
    "\n",
    "    # Check if the lengths of phi_values and psi_values match\n",
    "    if len(phi_values) != len(psi_values):\n",
    "        raise ValueError(\"The lengths of phi_values and psi_values must be the same.\")\n",
    "    \n",
    "    # Create a DataFrame from the phi_values and psi_values\n",
    "    data = pd.DataFrame({'Phi': phi_values, 'Psi': psi_values})\n",
    "    \n",
    "    # Create a density plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.kdeplot(data=data, x='Phi', y='Psi', fill=True, cmap='viridis', cbar=True)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Phi\")\n",
    "    plt.ylabel(\"Psi\")\n",
    "    plt.title(\"Phi vs. Psi Angles Density\")\n",
    "    save_path = f'model1_density_{iters}.png'\n",
    "    plt.savefig(save_path)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_concentrations(post_samples):\n",
    "    phi_concentration = post_samples['phi_conc']\n",
    "    psi_concentration = post_samples['psi_conc']\n",
    "    \n",
    "    # Fixing random state for reproducibility\n",
    "    np.random.seed(19680801)\n",
    "    \n",
    "    x = np.array(phi_concentration)\n",
    "    y = np.array(psi_concentration)\n",
    "    xlim = (x.min(), x.max())\n",
    "    ylim = (y.min(), y.max())\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    hb = plt.hexbin(x, y, gridsize=50, bins='log', cmap='inferno')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.title(\"Phi vs Psi Concentrations\")\n",
    "    cb = plt.colorbar(hb, label='counts')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr_scale_density(post_samples):\n",
    "    corr_scale = post_samples['corr_scale']\n",
    "    \n",
    "    # Create an Arviz InferenceData object\n",
    "    posterior_data = az.from_dict(\n",
    "        posterior={\n",
    "            \"corr_scale\": corr_scale\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Plot the density\n",
    "    az.style.use(\"arviz-doc\")\n",
    "    axes = az.plot_density(\n",
    "        posterior_data,\n",
    "        data_labels=[\"Posterior\"],\n",
    "        var_names=[\"corr_scale\"],\n",
    "        shade=0.2,\n",
    "    )\n",
    "\n",
    "    fig = axes.flatten()[0].get_figure()\n",
    "    fig.suptitle(\"Density Plot for Correlation Scale\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    N=1000 # Limit the data set size for testing\n",
    "    data = jnp.array(np.transpose(load_data(args.file_path)))[0:N]\n",
    "    \n",
    "    # Prepare to store predicted data\n",
    "    pred_datas = {}\n",
    "    \n",
    "    # Lists to store data for plotting\n",
    "    all_num_mix_comp = []\n",
    "    all_expected_waic = []\n",
    "    all_expected_loo = []\n",
    "    \n",
    "    # Set up random number generation\n",
    "    rng_key = random.PRNGKey(args.rng_seed)\n",
    "    \n",
    "    iters = 50 \n",
    "\n",
    "    for m in range(5, iters, 5):\n",
    "        # Split the random key into multiple keys for different purposes\n",
    "        rng_key, inf_key, pred_key = random.split(rng_key, 3)\n",
    "\n",
    "        # Determine the number of mixture components for this amino acid\n",
    "        num_mix_comp = m \n",
    "        \n",
    "        # Run Hamiltonian Monte Carlo to obtain posterior samples #, expected_loo, expected_ess, expected_rhat\n",
    "        posterior_samples, expected_waic, expected_loo, expected_ess, expected_rhat = run_hmc(inf_key, model, data, num_mix_comp, args)\n",
    "           \n",
    "        # Use the posterior samples to make predictions\n",
    "        predictive = Predictive(model, posterior_samples, parallel=True)\n",
    "        pred_datas[m] = predictive(pred_key, None, 1, num_mix_comp)[\"phi_psi\"].reshape(-1, 2)\n",
    "        \n",
    "        # Store values for plotting\n",
    "        all_num_mix_comp.append(num_mix_comp)\n",
    "        all_expected_waic.append(expected_waic)\n",
    "        all_expected_loo.append(expected_loo)\n",
    "        \n",
    "        # Sort num_comp and metric_values based on num_comp\n",
    "        sorted_data = sorted(zip(all_num_mix_comp, all_expected_waic, all_expected_loo))\n",
    "        sorted_num_mix_comp, sorted_expected_waic, sorted_expected_loo = zip(*sorted_data)\n",
    "        \n",
    "        # Plot the Ramachandran plot\n",
    "        phi_values = pred_datas[m][:, 0]\n",
    "        psi_values = pred_datas[m][:, 1]\n",
    "\n",
    "    plot_metric_vs_components(sorted_expected_waic, sorted_num_mix_comp, \"WAIC\", iters)\n",
    "    plot_metric_vs_components(sorted_expected_loo, sorted_num_mix_comp, \"LOO\", iters)\n",
    "    \n",
    "    plot_rama(phi_values, psi_values)\n",
    "    plot_density(phi_values, psi_values, iters)\n",
    "    plot_concentrations(posterior_samples)\n",
    "    plot_corr_scale_density(posterior_samples)\n",
    "    \n",
    "    print('expected_ess_std: ', np.std(expected_ess))\n",
    "    print('expected_rhat_mean: ', np.mean(expected_rhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Sine-skewed sine (bivariate von mises) mixture model\"\n",
    "    )\n",
    "    parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n",
    "    parser.add_argument(\"--num-warmup\", nargs=\"?\", default=500, type=int)\n",
    "    parser.add_argument(\"--rng_seed\", type=int, default=123)\n",
    "    parser.add_argument(\"--device\", default=\"gpu\", type=str, help='use \"cpu\" or \"gpu\".')\n",
    "    parser.add_argument(\"--file-path\", type=str, default=\"top500.txt\", help=\"Path to the data file.\")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
